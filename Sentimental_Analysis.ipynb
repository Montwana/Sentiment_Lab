{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f670e7-b342-4be8-9f7b-0051f9263130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\matsh\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\matsh\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\matsh\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\matsh\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\matsh\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Collecting pyprind\n",
      "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: click in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\matsh\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: pyprind\n",
      "Successfully installed pyprind-2.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn matplotlib nltk pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0660016c-2e96-4c0e-83c3-e674125f5386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IMDB dataset...\n",
      "100% | 80 MB | 0.12 MB/s | 693 sec elapsed\n",
      "Download complete. Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsh\\AppData\\Local\\Temp\\ipykernel_31100\\2852221307.py:35: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    if duration == 0:  # prevent division by zero\n",
    "        duration = 1e-6\n",
    "    progress_size = int(count * block_size)\n",
    "    if total_size > 0:\n",
    "        percent = count * block_size * 100.0 / total_size\n",
    "    else:\n",
    "        percent = 0\n",
    "    speed = progress_size / (1024.0**2 * duration)\n",
    "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n",
    "                     (percent, progress_size / (1024.0**2), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Download only if not already downloaded or extracted\n",
    "if not os.path.isdir('aclImdb'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    urllib.request.urlretrieve(source, target, reporthook)\n",
    "    print(\"\\nDownload complete. Extracting files...\")\n",
    "\n",
    "    with tarfile.open(target, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56e9720-2c38-4d60-8790-6f1a383fda80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:19:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to movie_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyprind\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "data = []  # store all reviews here\n",
    "\n",
    "for s in ('train', 'test'):\n",
    "    for label in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, label)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                data.append([txt, labels[label]])  # add row\n",
    "                pbar.update()\n",
    "\n",
    "# Convert once at the end â€” faster and clean\n",
    "df = pd.DataFrame(data, columns=['review', 'sentiment'])\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Saved to movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f053e-90fa-49c1-9a74-1c939a1a5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    # Find emoticons\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove non-word characters and add emoticons at the end\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c149b-2ba8-4150-9916-2233e0e1cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "])\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388295b-493c-48d7-8eb6-a674537f43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeade77a-2da4-4eda-a778-73122ae79de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part3\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcee309-8ab5-4e54-a486-f2639271ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "param_grid = [{\n",
    "    'vect__ngram_range': [(1, 1)],\n",
    "    'vect__stop_words': [stop, None],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [1.0, 10.0]\n",
    "}]\n",
    "\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(random_state=0, solver='liblinear'))\n",
    "])\n",
    "\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "print('Best Accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "print('Best Params:', gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916a1b5-99eb-42f8-9c29-525c223be643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314941a4-8e05-4980-a9df-becf70296b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de730e14-bdb1-45ca-b454-eaf7ecbada23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=123, learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82792608-737d-4922-b8c0-a8c1cbb086ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Example: Word cloud for topic 0\n",
    "topic_words = {feature_names[i]: lda.components_[0][i] for i in range(len(feature_names))}\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(topic_words)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Topic 1 Word Cloud\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91c8de-cf22-464c-a969-4a2aae79fb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
